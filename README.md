# Tiny-GPT: A From-Scratch Implementation of GPT-2

This repository contains a from-scratch implementation of GPT-2, created as an educational exercise. The model is currently trained on the TinyStories dataset, which is loaded from the HuggingFace datasets library, but you can easily use a different dataset by modifying `prepare.py`. TinyStories is a synthetic dataset of short stories generated by GPT-3.5/4 containing words that 3-4 year olds would typically understand. The dataset is structured as a dictionary with keys for the 'train' and 'validation' splits. The value associated with each key is a list of stories, where each story is a string of text. This format is typical with many of the datasets on Hugging Face, hence, the dataloaders should not require much, if any, alteration when swapping with another dataset from the datasets library.

## Configuration

The `train.py` file sets default hyperparameters and variables, and can be run without any additional arguments. To override these variables, you can pass `train.py` a JSON configuration file using the `--config_file your_config.json` flag. Additionally, you can override individual variables using the pattern `--variable_name value`. For example, `--device cuda`. Please note that arguments passed in individually take precedence over those given in the `--config_file`.

## Setup

### Data Preparation

The first step is to prepare the data and tokenisers. By default, the TinyStories dataset is used. Run the following command to prepare the data:

```bash
python data/tiny_stories/prepare.py
```

This command downloads the data to disk (if not already cached) and creates objects which only hold information of how to access the data in memory. Hence, running out of memory should not be a concern. If the TinyStories dataset is too large for your needs, you can set the percentage of the dataset to use by adjusting the `data_percentage` variable within `prepare.py` (which is set to 100 by default). This command further creates the tokeniser and saves it to a `tokeniser.pkl` file in the `data/tiny_stories/` directory. By default, the tokeniser is a simple character level encoding. However, a BPE tokeniser can also be selected by using the flag `--encoding_type` followed by `bpe` (or `simple` for the character level encoding). For example:

```bash
python data/tiny_stories/prepare.py --encoding_type bpe
```

The BPE tokeniser will be trained on the same subset of the dataset selected by data_percentage, producing a vocabulary of about 30,000 for the full dataset. The subsequence files will use the appropriate encoders and decoders so thereâ€™s no need to specify them again.


### Model Training

Next, train the model using the following command:

```bash
python train.py --config_file config/tiny_stories.py
```

This script supports CPU, CUDA, and MPS devices. The default device is CPU, but you can change this by specifying the `--device` flag. For example, to use CUDA, run:

```bash
python train.py --config_file config/tiny_stories --device cuda
```

Please note that `torch.compile` currently does not work with MPS.

As the model trains, it saves checkpoints to the `out-tiny-stories` directory.

## Generating Samples

Once you have trained the model, you can generate samples from it using the following command:

```bash
python sample.py --out_dir out-tiny-stories
```